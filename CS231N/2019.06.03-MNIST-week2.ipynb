{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初识卷积神经网络——MNIST\n",
    "## 1.课前碎碎念\n",
    "MNIST--手写数字识别被很多人称作是深度学习的“Hello World！”\n",
    "MNIST确实是一个简洁又简单的数据集，也是第一个深度学习算法解决的实际问题，\n",
    "MNIST数据集很小，很适合教学实用，同时资源占用较少，没有GPU也能够进行训练。\n",
    "\n",
    "我比较不喜欢“Hello World！”这个比喻，想要完全掌握MNIST，所需要的知识远不是敲进去一行代码就可以的。\n",
    "实际上GAN最早被提出时，也是在MNIST数据集上做的验证。\n",
    "\n",
    "在我有限的知识背景下，中心损失函数也是使用MNIST进行验证的。\n",
    "\n",
    "——江小河  课前碎碎念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.使用Keras编写Lenet\n",
    "首先我们导入算法框架工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as K\n",
    "# import keras as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我这里首先使用tensorflow后端的Keras进行讲解，注意这里我使用tensorflow.keras来引入keras。这是因为至少从1.9.0版本以后，tensorflow已经讲keras默认继承安装在了tensorflow包中。很多keras的教程会要求分别安装tensorflow和keras，通过import keras来导入。实际上没有任何区别（目前没有发现任何区别）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train`s shape is (60000, 28, 28)\n",
      "y_train`s shape is (60000,)\n",
      "x_test`s shape is (10000, 28, 28)\n",
      "y_test`s shape is (10000,)\n"
     ]
    }
   ],
   "source": [
    "def check_dimension(data, data_name):\n",
    "    assert type(data) is np.ndarray\n",
    "    print(data_name+\"`s shape is \" + str(data.shape))\n",
    "\n",
    "check_dimension(x_train, \"x_train\")\n",
    "check_dimension(y_train, \"y_train\")\n",
    "check_dimension(x_test, \"x_test\")\n",
    "check_dimension(y_test, \"y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num 43786 is an image for 9.\n",
      "Num 9839 is an image for 2.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def check_example(x, y, number=None):\n",
    "    if number == None:\n",
    "        number = np.random.randint(x.shape[0])\n",
    "    print(\"Num {} is an image for {}.\".format(number, y[number]))\n",
    "    plt.figure()\n",
    "    plt.imshow(x[number])\n",
    "\n",
    "check_example(x_train, y_train)\n",
    "check_example(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import one_hot\n",
    "y_train = K.backend.eval(one_hot(y_train, 10))\n",
    "y_test= K.backend.eval(one_hot(y_test, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train`s shape is (60000, 28, 28)\n",
      "y_train`s shape is (60000, 10)\n",
      "x_test`s shape is (10000, 28, 28)\n",
      "y_test`s shape is (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "check_dimension(x_train, \"x_train\")\n",
    "check_dimension(y_train, \"y_train\")\n",
    "check_dimension(x_test, \"x_test\")\n",
    "check_dimension(y_test, \"y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入Lenet模型图，作为创建网络的参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lenet](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1559582032465&di=4a4061c5062506fc7a4a4d605d2e2443&imgtype=0&src=http%3A%2F%2Fmmbiz.qpic.cn%2Fmmbiz_png%2F4lN1XOZshffJhuEYgHmB6aNaASYT9xjqANHsYIBsibQdynicOGUkqgNHBMbX0LHLhCiaswHvE0IltaZFIPWNJia9Yw%2F640%3Fwx_fmt%3Dpng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\AI\\Configuration\\Anaconda3\\envs\\CV-ND\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(6, kernel_size=(5, 5), padding=\"same\", activation=\"relu\", input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120, activation=\"relu\"))\n",
    "model.add(Dense(84, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看网络摘要，和Lenet进行对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 10, 10, 16)        2416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               48120     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编译模型（设置优化器，优化参数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = K.optimizers.Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mini_batch():\n",
    "    pass\n",
    "#keras 的训练函数可以进行batchsize的设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = K.backend.eval(K.backend.expand_dims(x_train, axis=-1))\n",
    "x_test = K.backend.eval(K.backend.expand_dims(x_test, axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\AI\\Configuration\\Anaconda3\\envs\\CV-ND\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 6.7444 - acc: 0.5472\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 3.9644 - acc: 0.7240\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 2.3455 - acc: 0.8211\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 1.7412 - acc: 0.8659\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 1.6989 - acc: 0.8740\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 1.6715 - acc: 0.8796\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 1.6521 - acc: 0.8840\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 1.6383 - acc: 0.8876\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 1.6280 - acc: 0.8902\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 1.6217 - acc: 0.8914\n"
     ]
    }
   ],
   "source": [
    "history_lenet_0 = model.fit(x_train, y_train, batch_size=64, epochs=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8835"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用模型在测试集上进行预测，生成结果为numpy.ndarray, shape=(10000, 10)\n",
    "y_test_predict = model.predict(x_test)\n",
    "#使用np.argmax可以求得预测样本标签，e.g. 1,2,3,...9\n",
    "y_label_pre = np.argmax(y_test_predict, axis=-1)\n",
    "#求测试准确度\n",
    "test_acc = np.sum((y_label_pre==np.argmax(y_test, axis=-1)).astype(int))/y_label_pre.shape[0]\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2233c863748>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_lenet_0.history\n",
    "plt.title(\"history_lenet_0\")\n",
    "plt.plot(history_lenet_0.history['acc'], c='r')\n",
    "plt.plot(history_lenet_0.history['loss'], c='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.TODO List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.修改网络参数进行测试，争取得到更好的结果。\n",
    "    如：网络深度，卷积核个数，卷积核尺寸，池化尺寸，全连接隐藏层节点数，归一化操作，batchsize，学习率，正则化参数……\n",
    "    （挑学会了的看懂了的改……）\n",
    "### 2.在修改网络之前，手推一遍自己的网络输出尺寸，参数量大小，使用summary函数进行验证。\n",
    "### 3.（选做）扩充版QMNIST数据集\n",
    "    github上推出了扩充版的MNIST数据集，下载并进行测试。\n",
    "    传送门：https://github.com/facebookresearch/qmnist\n",
    "    （可能需要GPU）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考VGG16构架缩减了卷积核数量，添加BacthNorm、Dropout层、GAP层        替换FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\AI\\Configuration\\Anaconda3\\envs\\CV-ND\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Conv2D(16, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", input_shape=(28,28,1)))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(16, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(MaxPooling2D((2,2)))\n",
    "model1.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(MaxPooling2D((2,2)))\n",
    "model1.add(Conv2D(64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(MaxPooling2D((2,2)))\n",
    "model1.add(Conv2D(128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(GlobalAveragePooling2D()) \n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Dense(32, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 28, 28, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 28, 28, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 3, 3, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 305,818\n",
      "Trainable params: 304,858\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 2.1096 - acc: 0.7129\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 1.0726 - acc: 0.9582\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.7425 - acc: 0.9786\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.5331 - acc: 0.9866\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3977 - acc: 0.9906\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3100 - acc: 0.9926\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.2491 - acc: 0.9944\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.2077 - acc: 0.9958\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.1787 - acc: 0.9974\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.1579 - acc: 0.9978\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.1428 - acc: 0.9981\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.1319 - acc: 0.9984\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.1237 - acc: 0.9984\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.1166 - acc: 0.9987\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.1103 - acc: 0.9986\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.1036 - acc: 0.9992\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0989 - acc: 0.9994\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0948 - acc: 0.9994\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0912 - acc: 0.9993\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0881 - acc: 0.9995\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0843 - acc: 0.9996\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0816 - acc: 0.9997\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0792 - acc: 0.9996\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0772 - acc: 0.9997\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0749 - acc: 0.9995\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0729 - acc: 0.9997\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0706 - acc: 0.9996\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0688 - acc: 0.9996\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0672 - acc: 0.9997\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0656 - acc: 0.9998\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0640 - acc: 0.9997\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0625 - acc: 0.9997\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0618 - acc: 0.9996\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0610 - acc: 0.9995\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0622 - acc: 0.9988\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0683 - acc: 0.9968\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0706 - acc: 0.9961\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0670 - acc: 0.9967\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0626 - acc: 0.9977\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0581 - acc: 0.9987\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0558 - acc: 0.9990\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0532 - acc: 0.9995\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0517 - acc: 0.9996\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0499 - acc: 0.9997\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0494 - acc: 0.9998\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0482 - acc: 0.9997\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0475 - acc: 0.9997\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0468 - acc: 0.9997\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0461 - acc: 0.9997\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0453 - acc: 0.9997\n"
     ]
    }
   ],
   "source": [
    "adam = K.optimizers.Adam(lr=0.001, decay=1e-5, epsilon=1e-08)\n",
    "model1.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_lenet_0 = model1.fit(x_train, y_train, batch_size=2048, epochs=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9951"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predict = model1.predict(x_test)\n",
    "y_label_pre = np.argmax(y_test_predict, axis=-1)\n",
    "test_acc = np.sum((y_label_pre==np.argmax(y_test, axis=-1)).astype(int))/y_label_pre.shape[0]\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2:\n",
    "|Layer|Output Shape| Parm|\n",
    "|---|---|---|\n",
    "|CONV1-1| `28 * 28 * 16`| `(3*3+1) * 16`|\n",
    "|CONV1-2| `28 * 28 * 16`| `(3*3*16+1) * 16`|\n",
    "|MAX POOL1| `14 * 14 * 16`| 0 |\n",
    "|CONV2-1| `14 * 14 * 32`| `(3*3*16+1) * 32`|\n",
    "|CONV2-2| `14 * 14 * 32`| `(3*3*32+1) * 32`|\n",
    "|MAX POOL2| `7 * 7 * 32`| 0 |\n",
    "|CONV2-1| `7 * 7 * 64`| `(3*3*32+1) * 64`|\n",
    "|CONV2-2| `7 * 7 * 64`| `(3*3*64+1) * 64`|\n",
    "|MAX POOL3| `7 * 7 * 32`| 0 |\n",
    "|CONV3-1| `3 * 3 * 128`| `(3*3*64+1) * 128`|\n",
    "|CONV3-2| `3 * 3 * 128`| `(3*3*128+1) * 128`|\n",
    "|GAP| `1 * 128`| 0 |\n",
    "|FC1|`1*64`|`(128+1) * 64`|\n",
    "|FC2|`1*32`|`(64+1) * 32`|\n",
    "|FC3|`1*10`|`(32+1) * 10`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train,y_train,test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import  ReduceLROnPlateau\n",
    "lr = ReduceLROnPlateau(monitor='val_acc',\n",
    "                      patience=3,\n",
    "                      verbose=1,\n",
    "                      factor=0.7,\n",
    "                      min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "6000/6000 [==============================] - 1s 125us/sample - loss: 0.1386 - acc: 0.9762\n",
      "27/27 [==============================] - 11s 402ms/step - loss: 0.2075 - acc: 0.9591 - val_loss: 0.1387 - val_acc: 0.9762\n",
      "Epoch 2/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.1202 - acc: 0.9807\n",
      "27/27 [==============================] - 9s 344ms/step - loss: 0.1287 - acc: 0.9793 - val_loss: 0.1199 - val_acc: 0.9807\n",
      "Epoch 3/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0832 - acc: 0.9890\n",
      "27/27 [==============================] - 10s 367ms/step - loss: 0.1165 - acc: 0.9832 - val_loss: 0.0833 - val_acc: 0.9890\n",
      "Epoch 4/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0763 - acc: 0.9912\n",
      "27/27 [==============================] - 10s 368ms/step - loss: 0.1036 - acc: 0.9857 - val_loss: 0.0763 - val_acc: 0.9912\n",
      "Epoch 5/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0660 - acc: 0.9922\n",
      "27/27 [==============================] - 10s 373ms/step - loss: 0.0954 - acc: 0.9873 - val_loss: 0.0661 - val_acc: 0.9922\n",
      "Epoch 6/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0765 - acc: 0.9910\n",
      "27/27 [==============================] - 10s 368ms/step - loss: 0.0946 - acc: 0.9874 - val_loss: 0.0765 - val_acc: 0.9910\n",
      "Epoch 7/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0690 - acc: 0.9913\n",
      "27/27 [==============================] - 10s 379ms/step - loss: 0.0886 - acc: 0.9888 - val_loss: 0.0691 - val_acc: 0.9913\n",
      "Epoch 8/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0657 - acc: 0.9920\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "27/27 [==============================] - 10s 382ms/step - loss: 0.0856 - acc: 0.9891 - val_loss: 0.0658 - val_acc: 0.9920\n",
      "Epoch 9/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0541 - acc: 0.9953\n",
      "27/27 [==============================] - 10s 362ms/step - loss: 0.0836 - acc: 0.9896 - val_loss: 0.0542 - val_acc: 0.9953\n",
      "Epoch 10/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0560 - acc: 0.9945\n",
      "27/27 [==============================] - 10s 376ms/step - loss: 0.0790 - acc: 0.9906 - val_loss: 0.0562 - val_acc: 0.9945\n",
      "Epoch 11/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0563 - acc: 0.9952\n",
      "27/27 [==============================] - 10s 375ms/step - loss: 0.0754 - acc: 0.9907 - val_loss: 0.0565 - val_acc: 0.9952\n",
      "Epoch 12/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0523 - acc: 0.9953\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "27/27 [==============================] - 10s 377ms/step - loss: 0.0753 - acc: 0.9917 - val_loss: 0.0524 - val_acc: 0.9953\n",
      "Epoch 13/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0561 - acc: 0.9950\n",
      "27/27 [==============================] - 10s 383ms/step - loss: 0.0746 - acc: 0.9914 - val_loss: 0.0562 - val_acc: 0.9950\n",
      "Epoch 14/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0531 - acc: 0.9950\n",
      "27/27 [==============================] - 10s 379ms/step - loss: 0.0711 - acc: 0.9921 - val_loss: 0.0533 - val_acc: 0.9950\n",
      "Epoch 15/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0554 - acc: 0.9957\n",
      "27/27 [==============================] - 10s 375ms/step - loss: 0.0691 - acc: 0.9925 - val_loss: 0.0555 - val_acc: 0.9957\n",
      "Epoch 16/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0498 - acc: 0.9957\n",
      "27/27 [==============================] - 10s 372ms/step - loss: 0.0698 - acc: 0.9923 - val_loss: 0.0501 - val_acc: 0.9957\n",
      "Epoch 17/80\n",
      "6000/6000 [==============================] - 0s 24us/sample - loss: 0.0503 - acc: 0.9953\n",
      "27/27 [==============================] - 10s 380ms/step - loss: 0.0687 - acc: 0.9925 - val_loss: 0.0505 - val_acc: 0.9953\n",
      "Epoch 18/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0497 - acc: 0.9960\n",
      "27/27 [==============================] - 10s 371ms/step - loss: 0.0681 - acc: 0.9926 - val_loss: 0.0498 - val_acc: 0.9960\n",
      "Epoch 19/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0519 - acc: 0.9960\n",
      "27/27 [==============================] - 10s 370ms/step - loss: 0.0671 - acc: 0.9926 - val_loss: 0.0520 - val_acc: 0.9960\n",
      "Epoch 20/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0510 - acc: 0.9945\n",
      "27/27 [==============================] - 10s 384ms/step - loss: 0.0676 - acc: 0.9927 - val_loss: 0.0511 - val_acc: 0.9945\n",
      "Epoch 21/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0477 - acc: 0.9963\n",
      "27/27 [==============================] - 10s 374ms/step - loss: 0.0665 - acc: 0.9931 - val_loss: 0.0478 - val_acc: 0.9963\n",
      "Epoch 22/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0609 - acc: 0.9927\n",
      "27/27 [==============================] - 10s 375ms/step - loss: 0.0664 - acc: 0.9927 - val_loss: 0.0610 - val_acc: 0.9927\n",
      "Epoch 23/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0529 - acc: 0.9950\n",
      "27/27 [==============================] - 10s 374ms/step - loss: 0.0642 - acc: 0.9932 - val_loss: 0.0530 - val_acc: 0.9950\n",
      "Epoch 24/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0518 - acc: 0.9953\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "27/27 [==============================] - 10s 375ms/step - loss: 0.0620 - acc: 0.9937 - val_loss: 0.0520 - val_acc: 0.9953\n",
      "Epoch 25/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0481 - acc: 0.9960\n",
      "27/27 [==============================] - 10s 375ms/step - loss: 0.0628 - acc: 0.9935 - val_loss: 0.0483 - val_acc: 0.9960\n",
      "Epoch 26/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0511 - acc: 0.9948\n",
      "27/27 [==============================] - 10s 371ms/step - loss: 0.0626 - acc: 0.9933 - val_loss: 0.0512 - val_acc: 0.9948\n",
      "Epoch 27/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0468 - acc: 0.9958\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "27/27 [==============================] - 10s 371ms/step - loss: 0.0615 - acc: 0.9938 - val_loss: 0.0470 - val_acc: 0.9958\n",
      "Epoch 28/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0473 - acc: 0.9957\n",
      "27/27 [==============================] - 10s 366ms/step - loss: 0.0614 - acc: 0.9942 - val_loss: 0.0475 - val_acc: 0.9957\n",
      "Epoch 29/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0470 - acc: 0.9963\n",
      "27/27 [==============================] - 10s 371ms/step - loss: 0.0595 - acc: 0.9946 - val_loss: 0.0472 - val_acc: 0.9963\n",
      "Epoch 30/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0471 - acc: 0.9962\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00016806999628897755.\n",
      "27/27 [==============================] - 10s 371ms/step - loss: 0.0597 - acc: 0.9943 - val_loss: 0.0472 - val_acc: 0.9962\n",
      "Epoch 31/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0463 - acc: 0.9967\n",
      "27/27 [==============================] - 10s 364ms/step - loss: 0.0583 - acc: 0.9943 - val_loss: 0.0465 - val_acc: 0.9967\n",
      "Epoch 32/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0467 - acc: 0.9962\n",
      "27/27 [==============================] - 10s 369ms/step - loss: 0.0573 - acc: 0.9944 - val_loss: 0.0469 - val_acc: 0.9962\n",
      "Epoch 33/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0477 - acc: 0.9957\n",
      "27/27 [==============================] - 10s 377ms/step - loss: 0.0564 - acc: 0.9950 - val_loss: 0.0479 - val_acc: 0.9957\n",
      "Epoch 34/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0471 - acc: 0.9967\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.00011764899536501615.\n",
      "27/27 [==============================] - 10s 371ms/step - loss: 0.0580 - acc: 0.9944 - val_loss: 0.0473 - val_acc: 0.9967\n",
      "Epoch 35/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0466 - acc: 0.9963\n",
      "27/27 [==============================] - 10s 368ms/step - loss: 0.0567 - acc: 0.9948 - val_loss: 0.0469 - val_acc: 0.9963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0464 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 367ms/step - loss: 0.0553 - acc: 0.9951 - val_loss: 0.0466 - val_acc: 0.9965\n",
      "Epoch 37/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0463 - acc: 0.9967\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 8.235429777414538e-05.\n",
      "27/27 [==============================] - 10s 374ms/step - loss: 0.0561 - acc: 0.9950 - val_loss: 0.0465 - val_acc: 0.9967\n",
      "Epoch 38/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0464 - acc: 0.9967\n",
      "27/27 [==============================] - 10s 373ms/step - loss: 0.0564 - acc: 0.9950 - val_loss: 0.0466 - val_acc: 0.9967\n",
      "Epoch 39/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0468 - acc: 0.9967\n",
      "27/27 [==============================] - 10s 370ms/step - loss: 0.0560 - acc: 0.9947 - val_loss: 0.0470 - val_acc: 0.9967\n",
      "Epoch 40/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0464 - acc: 0.9963\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 5.76480058953166e-05.\n",
      "27/27 [==============================] - 10s 375ms/step - loss: 0.0560 - acc: 0.9950 - val_loss: 0.0466 - val_acc: 0.9963\n",
      "Epoch 41/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0463 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 371ms/step - loss: 0.0561 - acc: 0.9949 - val_loss: 0.0465 - val_acc: 0.9965\n",
      "Epoch 42/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0462 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 369ms/step - loss: 0.0548 - acc: 0.9953 - val_loss: 0.0464 - val_acc: 0.9965\n",
      "Epoch 43/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0461 - acc: 0.9965\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 4.0353603617404586e-05.\n",
      "27/27 [==============================] - 10s 369ms/step - loss: 0.0538 - acc: 0.9951 - val_loss: 0.0463 - val_acc: 0.9965\n",
      "Epoch 44/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0461 - acc: 0.9967\n",
      "27/27 [==============================] - 10s 369ms/step - loss: 0.0559 - acc: 0.9950 - val_loss: 0.0463 - val_acc: 0.9967\n",
      "Epoch 45/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0459 - acc: 0.9963\n",
      "27/27 [==============================] - 10s 374ms/step - loss: 0.0550 - acc: 0.9949 - val_loss: 0.0461 - val_acc: 0.9963\n",
      "Epoch 46/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0462 - acc: 0.9963\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 2.8247522277524694e-05.\n",
      "27/27 [==============================] - 10s 379ms/step - loss: 0.0556 - acc: 0.9949 - val_loss: 0.0464 - val_acc: 0.9963\n",
      "Epoch 47/80\n",
      "6000/6000 [==============================] - 0s 46us/sample - loss: 0.0460 - acc: 0.9963\n",
      "27/27 [==============================] - 10s 375ms/step - loss: 0.0551 - acc: 0.9949 - val_loss: 0.0461 - val_acc: 0.9963\n",
      "Epoch 48/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0459 - acc: 0.9962\n",
      "27/27 [==============================] - 10s 368ms/step - loss: 0.0574 - acc: 0.9949 - val_loss: 0.0461 - val_acc: 0.9962\n",
      "Epoch 49/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0459 - acc: 0.9963\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.977326610358432e-05.\n",
      "27/27 [==============================] - 10s 370ms/step - loss: 0.0564 - acc: 0.9944 - val_loss: 0.0461 - val_acc: 0.9963\n",
      "Epoch 50/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0462 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 368ms/step - loss: 0.0543 - acc: 0.9952 - val_loss: 0.0464 - val_acc: 0.9965\n",
      "Epoch 51/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0461 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 371ms/step - loss: 0.0545 - acc: 0.9952 - val_loss: 0.0463 - val_acc: 0.9965\n",
      "Epoch 52/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0461 - acc: 0.9965\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.3841286272509023e-05.\n",
      "27/27 [==============================] - 10s 368ms/step - loss: 0.0568 - acc: 0.9948 - val_loss: 0.0463 - val_acc: 0.9965\n",
      "Epoch 53/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0459 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 377ms/step - loss: 0.0536 - acc: 0.9952 - val_loss: 0.0461 - val_acc: 0.9965\n",
      "Epoch 54/80\n",
      "6000/6000 [==============================] - 0s 26us/sample - loss: 0.0458 - acc: 0.9963\n",
      "27/27 [==============================] - 10s 371ms/step - loss: 0.0549 - acc: 0.9953 - val_loss: 0.0459 - val_acc: 0.9963\n",
      "Epoch 55/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0459 - acc: 0.9963\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "27/27 [==============================] - 10s 371ms/step - loss: 0.0554 - acc: 0.9953 - val_loss: 0.0461 - val_acc: 0.9963\n",
      "Epoch 56/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0460 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 369ms/step - loss: 0.0559 - acc: 0.9952 - val_loss: 0.0462 - val_acc: 0.9965\n",
      "Epoch 57/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0460 - acc: 0.9967\n",
      "27/27 [==============================] - 10s 371ms/step - loss: 0.0548 - acc: 0.9950 - val_loss: 0.0462 - val_acc: 0.9967\n",
      "Epoch 58/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0459 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 365ms/step - loss: 0.0538 - acc: 0.9953 - val_loss: 0.0461 - val_acc: 0.9965\n",
      "Epoch 59/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0460 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 369ms/step - loss: 0.0549 - acc: 0.9951 - val_loss: 0.0462 - val_acc: 0.9965\n",
      "Epoch 60/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0459 - acc: 0.9967\n",
      "27/27 [==============================] - 10s 380ms/step - loss: 0.0528 - acc: 0.9956 - val_loss: 0.0461 - val_acc: 0.9967\n",
      "Epoch 61/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0459 - acc: 0.9967\n",
      "27/27 [==============================] - 10s 375ms/step - loss: 0.0535 - acc: 0.9955 - val_loss: 0.0461 - val_acc: 0.9967\n",
      "Epoch 62/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0458 - acc: 0.9967\n",
      "27/27 [==============================] - 10s 367ms/step - loss: 0.0549 - acc: 0.9951 - val_loss: 0.0460 - val_acc: 0.9967\n",
      "Epoch 63/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0459 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 376ms/step - loss: 0.0545 - acc: 0.9955 - val_loss: 0.0461 - val_acc: 0.9965\n",
      "Epoch 64/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0459 - acc: 0.9963\n",
      "27/27 [==============================] - 10s 382ms/step - loss: 0.0534 - acc: 0.9955 - val_loss: 0.0461 - val_acc: 0.9963\n",
      "Epoch 65/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0458 - acc: 0.9963\n",
      "27/27 [==============================] - 10s 369ms/step - loss: 0.0545 - acc: 0.9951 - val_loss: 0.0460 - val_acc: 0.9963\n",
      "Epoch 66/80\n",
      "6000/6000 [==============================] - 0s 26us/sample - loss: 0.0458 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 382ms/step - loss: 0.0538 - acc: 0.9954 - val_loss: 0.0459 - val_acc: 0.9965\n",
      "Epoch 67/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0460 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 387ms/step - loss: 0.0540 - acc: 0.9951 - val_loss: 0.0462 - val_acc: 0.9965\n",
      "Epoch 68/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0460 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 386ms/step - loss: 0.0530 - acc: 0.9954 - val_loss: 0.0462 - val_acc: 0.9965\n",
      "Epoch 69/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0461 - acc: 0.9967\n",
      "27/27 [==============================] - 10s 387ms/step - loss: 0.0544 - acc: 0.9955 - val_loss: 0.0463 - val_acc: 0.9967\n",
      "Epoch 70/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0461 - acc: 0.9965\n",
      "27/27 [==============================] - 11s 392ms/step - loss: 0.0523 - acc: 0.9956 - val_loss: 0.0463 - val_acc: 0.9965\n",
      "Epoch 71/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0460 - acc: 0.9965\n",
      "27/27 [==============================] - 11s 397ms/step - loss: 0.0525 - acc: 0.9954 - val_loss: 0.0462 - val_acc: 0.9965\n",
      "Epoch 72/80\n",
      "6000/6000 [==============================] - 0s 21us/sample - loss: 0.0459 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 379ms/step - loss: 0.0553 - acc: 0.9950 - val_loss: 0.0461 - val_acc: 0.9965\n",
      "Epoch 73/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0460 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 383ms/step - loss: 0.0535 - acc: 0.9954 - val_loss: 0.0462 - val_acc: 0.9965\n",
      "Epoch 74/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0459 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 367ms/step - loss: 0.0538 - acc: 0.9953 - val_loss: 0.0461 - val_acc: 0.9965\n",
      "Epoch 75/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0459 - acc: 0.9963\n",
      "27/27 [==============================] - 10s 367ms/step - loss: 0.0543 - acc: 0.9951 - val_loss: 0.0461 - val_acc: 0.9963\n",
      "Epoch 76/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0459 - acc: 0.9963\n",
      "27/27 [==============================] - 10s 368ms/step - loss: 0.0522 - acc: 0.9956 - val_loss: 0.0461 - val_acc: 0.9963\n",
      "Epoch 77/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0460 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 376ms/step - loss: 0.0541 - acc: 0.9955 - val_loss: 0.0462 - val_acc: 0.9965\n",
      "Epoch 78/80\n",
      "6000/6000 [==============================] - 0s 22us/sample - loss: 0.0460 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 374ms/step - loss: 0.0533 - acc: 0.9953 - val_loss: 0.0462 - val_acc: 0.9965\n",
      "Epoch 79/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0460 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 371ms/step - loss: 0.0551 - acc: 0.9952 - val_loss: 0.0462 - val_acc: 0.9965\n",
      "Epoch 80/80\n",
      "6000/6000 [==============================] - 0s 23us/sample - loss: 0.0460 - acc: 0.9965\n",
      "27/27 [==============================] - 10s 378ms/step - loss: 0.0532 - acc: 0.9958 - val_loss: 0.0462 - val_acc: 0.9965\n"
     ]
    }
   ],
   "source": [
    "gen = ImageDataGenerator(rotation_range = 10,\n",
    "                        width_shift_range = 0.2,\n",
    "                        height_shift_range = 0.2,\n",
    "                        shear_range = 0.3,\n",
    "                        zoom_range = 0.2,\n",
    "                        data_format = 'channels_last',)\n",
    "gen.fit(x_train)\n",
    "train_generator = gen.flow(x_train, y_train, batch_size=2048)\n",
    "history = model1.fit(train_generator,\n",
    "                    epochs = 80,\n",
    "                    validation_data = (x_val, y_val),\n",
    "                    callbacks=[lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predict = model1.predict(x_test)\n",
    "y_label_pre = np.argmax(y_test_predict, axis=-1)\n",
    "test_acc = np.sum((y_label_pre==np.argmax(y_test, axis=-1)).astype(int))/y_label_pre.shape[0]\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('D:/AI/workspace/CV/CS231N/model.h5')\n",
    "new_model = K.models.load_model('D:/AI/workspace/CV/CS231N/model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:CV-ND] *",
   "language": "python",
   "name": "conda-env-CV-ND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
